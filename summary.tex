\documentclass[12pt]{amsart}
\usepackage[text={7in,10in}, centering]{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{CS 109 Course Summary}
\author{Daniel Jackoway}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

\section{Counting}
\subsection{Recursive combination definition}
\begin{align*}
{n \choose k} &= {n-1 \choose k-1} + {n-1 \choose k} \\
\end{align*}

\subsection{Multinomial}
\begin{align*}
&\text{n trials of an experiment. Each results in m outcomes with respective probability } p_1,p_2, ... p_m \\
&X_i = \text{number of trials with outcome i} \\
&P(X_1 = c_1, X_2 = c_2, ... X_m = c_m) = {n \choose c_1 c_2 ... c_m} p_1^{c_1} p_2^{c_2} ... p_m^{c_m}
\end{align*}
\section{Probability Properties}
%
\subsection{Conditional Probability}
\[
P(E \mid F) = {P(EF) \over P(F)}
\]
\subsection{Chain Rule}
Implied by conditional probability
\begin{align*}
&P(EF) = P(E \mid F) P(F) \\
&\text{more generally:} \\
&P(E_1 E_2 E_3 ... E_n) = P(E_1) P(E_2 \mid E_1) P(E_3 \mid E_1 E_2) ... P(E_n \mid E_1 E_2 ... E_{n-1})
\end{align*}
%
\subsection{Bayes' Theorem} Lots of forms
\begin{align*}
P(F \mid E) &= {P(EF) \over P(E)} \\
&= {P(E \mid F) P(F) \over P(E)} \\
&= {P(E \mid F) P(F) \over P(E \mid F) P(F) + P(E \mid F^c) P(F^c)} \\
P(E) &= P(EF) + P(EF^c) \\
&= P(E \mid F) P(F) + P(E \mid F^c) P(F^c)
\end{align*}

\subsection{Odds}
\begin{align*}
&{P(A) \over P(A^c)} = {P(a) \over 1-P(A)} \\
&\text{After observing E, updated odds by multiplying by } {P(E \mid A) \over P(E \mid A^c)}
\end{align*}

\subsection{Conditional}
Can condition consistently on an event and commutativity, chain rule, intersection, and Bonferroni all still hold:
\begin{align*}
P(AB \mid E) &= P(BA \mid E) \\
P(AB \mid E) &= P(A \mid BE) P(B \mid E) \\
P(A \mid BE) &= {P(B \mid AE) P(A \mid E) \over P(B \mid E)}
\end{align*}

\subsection{Dependence}
E and F are independent iff $P(EF) = P(E) P(F)$. More generally:\\
$E_1, E_2, ..., E_n$ are independent iff for every subset $E_1 ... E_r$ (where $ r \leq n$), $P(E_1 E_2 ... E_r) = P(E_1) P(E_2) ... P(E_r)$

\section{Random Variables}
\subsection{Variance}
\begin{align*}
Var(X) &= E[(X - \mu)^2] = E[X^2] - (E[X])^2 \\
Var(aX + b) &= a^2 Var(X)
\end{align*}
%
\subsection{Binomial}
\subsubsection{What}
\[
X \sim \text{Bin}(n,p)
\]
Do n independent trials with probability of success p. X is the number of successes.
\subsubsection{Formulae}
\[
PMF: P(X = i) = p(i) = {n \choose i} p^i (1-p)^{n-i}
\]
\[
\text{CMF: sum the PMF from 0 to i}
\]
\[
E[X] = np
\]
\[
Var(X) = np(1-p)
\]
\subsubsection{Combining}
If they have the \textbf{same p}, you add their n's when you add them.
%
\subsection{Negative Binomial}
\subsubsection{What}
\[
X \sim NegBin(r, p)
\]
Do independent trials with probability p of success until r successes. X is the number of trials it takes.
\subsubsection{Formulae}
\[
P(X = n) = {n - 1 \choose r - 1} p^r (1-p)^{n-r}
\]
\[
E[X] = {r \over p}
\]
\[
Var(X) = {r(1-p) \over p^2}
\]
%
\subsection{Normal}
\subsubsection{What}
aka Gaussian. PDF is symmetric around $\mu$, peaks at $\mu$, ranges $-\infty, \infty$. Often comes from sum of multiple vars. Approximates binomial, best when $p = .5$
\subsubsection{Formulae}
\begin{align*}
X &\sim N(\mu, \sigma^2) \\
PDF: f(x) &= {1 \over \sigma \sqrt{2\pi}} e^{-(x-\mu)^2 / 2 \sigma^2} \\
E[X] &= \mu \\
Var(X) &= \sigma^2 \\
Bin(n, p) &\approx N(np, np(1-p)) & \text{if } np(1-p) \geq 10
\end{align*}
\subsubsection{Combining}
\[
aN(\mu_1, \sigma_1^2) - bN(\mu_2, \sigma_2^2) = N(a\mu_1 - b\mu_2, a^2 \sigma_1^2 + b^2 \sigma_2^2)
\]
\subsubsection{Using standard normal table}
\begin{align*}
Z &\sim N(1,0) \\
P(Z < n)&= P(Z \leq n) = \Phi(n) \\
X &\sim N(\mu, \sigma^2) \\
P(X < n) &= P({X - \mu \over \sigma} < P({n - \mu \over \sigma}) = P(Z < {n - \mu \over \sigma}) = \Phi({n - \mu \over \sigma}) \\
P(X > n) &= P((X < n)^c) = 1 - \Phi({n - \mu \over \sigma}) \\
\Phi(-z) &= P(Z \leq -z) = P(Z \geq z) = 1 - \Phi(z) & \text{by symmetry}
\end{align*}
\subsubsection{Continuity Correction}
Use when approximating a discrete distribution (eg. binomial) with a continuous. Go either up or down by .5 to make up for the fact that it's continuous.
\begin{align*}
X &\sim Bin(1000, .5) \approx Y \sim N(500, 250)\\
P(X \leq 499) &= P(X < 500) \approx P(Y < 499.5) \\
P(X > 500) &= P(X \geq 499) \approx P(Y > 499.5)
\end{align*}
%
\subsection{Exponential}
\subsubsection{What}
Models time until an event happens. "Memoryless"--distribution looking forward isn't affected by how long since the last event.
\subsubsection{Formulae}
\begin{align*}
X &\sim Exp(\lambda) \\
PDF: f(x) &= \lambda e^{-\lambda x} & \text{if x} \geq 0 \text{ (else 0)} \\
CDF: F(x) &= P(X \leq x) = 1 - e^{-\lambda x} & \text{where x} \geq 0 \\
E[X] &= {1 \over \lambda} \\
Var(X) &= {1 \over \lambda^2}
\end{align*}
%
\subsection{Geometric}
\subsubsection{What}
X is the number of independent trials (with probability p of success) until first success. $X \sim NegBin(1, p)$ See Negative Binomial.
\[
X \sim Geo(p)
\]
\subsubsection{Formulae}
\begin{align*}
P(X = n) &= (1-p)^{n-1} p \\
E[X] &= {1 \over p} \\
Var(X) &= {1 - p \over p^2}
\end{align*}
%
\subsection{Hypergeometric}
\subsubsection{What}
Urn with N balls, m white. Draw n balls without replacement. X is number of white balls drawn.
\[
X \sim HypG(n, N, m)
\]
\subsubsection{Formulae}
\begin{align*}
PMF: P(X = i) &= { {m \choose i} {N-m \choose n-i} \over {N \choose n}} &i=0,1,...,n \\
E[X] &= n \left( {m \over N} \right) \\
Var(X) &= {nm(N-n)(N-m) \over N^2 (N-1)} \\
\text{as } n &\to \infty, HypG(n, N, m) \to Bin(n, m/N)
\end{align*}
%
\subsection{Poisson}
\subsubsection{What}
$\lambda$ is the rate. eg. $\lambda = 5 \to 5$ occurrences happening per time period. Also approximates the binomial distribution when n is large, p is small, and $\lambda = np$ is moderate. In fact, the limit as n goes to infinity and p goes to 0 of the binomial is the poisson
\[
X \sim Poi(\lambda)
\]
\subsubsection{Formulae}
\begin{align*}
PMF: P(X=i) &= e^{-\lambda} {\lambda^i \over i!} \\
Bin(n, p) &\approx Poi(np) & \text{if } n > 100, p < .1 \text{ or } n > 20, p < .05
\end{align*}
\subsubsection{Combining}
Sum the $\lambda$'s when summing poisson RV's
%
\subsection{Uniform}
\subsubsection{What}
X is equally likely to be any number between $\alpha$ and $\beta$
\[
X \sim Uni(\alpha, \beta)
\]
\subsubsection{Formulae}
\begin{align*}
PDF: f(x) &= {1 \over \beta - \alpha} & \alpha < x < \beta \text{ (else 0)} \\
P(a \leq x \leq b) &= {b - a \over \beta - \alpha} \\
E[X] &= {\alpha + \beta \over 2} \\
Var(X) &= {(\beta - \alpha)^2 \over 12}
\end{align*}
%
\subsection{Beta}
\subsubsection{What}
Weird. Adapts as you change the params.\\
One use: flip a coin with unknown probability of coming up heads. Get n heads and m tails. The distribution suggested by that is $X \sim Beta(n + 1, m + 1)$. \\
symmetric when a = b.  High a shifts density right, high b shifts density left.
\[
X \sim Beta(a,b)
\]
\subsubsection{Formulae}
\begin{align*}
B(a,b) &= \int_0^1 x^{a-1} (1-x)^{b-1} dx \\
f(x) &= {1 \over B(a,b)} x^{a-1} (1-x)^{b-1} & 0 < x < 1 \text{ (else }0)\\
E[X] &= {a \over a + b} \\
Var(X) &= {ab \over (a + b)^2 (a + b + 1)}
\end{align*}
%
\subsection{Bernoulli}
\subsubsection{What}
1 with probability p and 0 with probability 1-p
\[
X \sim Ber(p)
\]
\subsubsection{Formulae}
\begin{align*}
E[X] &= p \\
Var(X) &= p(1-p)
\end{align*}

\section{Two Variables}
\subsection{Definitions}
\subsubsection{Joint PMF (Discrete)}
\begin{align*}
p_{X,Y}(a,b) = P(X = a, Y = b)
\end{align*}
\subsubsection{Marginal Distribution (Discrete)}
isolates to 1 variable by summing over the other
\begin{align*}
p_X(a) &= P(X = a) = \sigma_y p_{x,y}(a,y) \\
p_Y(b) &= P(Y = b) = \sigma_x p_{x,y}(x,b)
\end{align*}
\subsubsection{Joint CDF (Continuous)}
\[
F_{X,Y}(a,b) = P(X \leq a, Y \leq b)
\]
\subsubsection{Marginal Distribution (Continuous)}
\begin{align*}
F_X(a) &= P(X \leq a) = P(X \leq a, Y < \infty) = F_{X,Y}(a,\infty) \\
F_Y(b) &= P(Y \leq b) = P(X < \infty, Y \leq a) = F_{X,Y}(\infty, b)
\end{align*}
\subsection{Independence}
Discrete RV's X and Y are independent iff
\begin{align*}
p(x,y) &= p_X(x)p_Y(y) & \text{for all x,y}
\end{align*}

Continuous RV's X and Y are independent iff
\begin{align*}
P(X \leq a, Y \leq b) &= P(X\leq a)p(Y \leq b) & \text{for any a,b}
\end{align*}

\subsection{Expectation}
Sum/integrate the variable times the marginal density for all possible values of the variable. Eg. continuous:
\[
E[X] = \int_{-\infty}^{\infty} xf_X(x) dx
\]
\section{Linearity of Expectation}
\[
E\left[\sum_i X_i\right] = \sum_i E[X_i]
\]
regardless of dependence between the $X_i$'s. Very useful.
\section{Covariance and Correlation}
\subsection{Covariance}
\begin{align*}
&Cov(X,Y) = E[XY] - E[X]E[Y] \\
&Cov(X,X) = E[X^2] - (E[X])^2 = Var(X) \\
&Cov(aX+b, Y) = a Cov(X,Y)\\
&\text{if X and Y are independent } Cov(X,Y)=0 \\
&Cov \left(\sum_{i=1}^n X_i, \sum_{j=1}^n Y_j\right) = \sum_{i,j} Cov(X_i, Y_j)
\end{align*}
%
\subsection{Correlation}
\[
\rho(X,Y) = {Cov(X,Y) \over \sqrt{Var(X) Var(Y)}}
\]
%
\section{Bias \& Consistency}
\subsection{Bias}
An estimator is unbiased iff $E[\hat{\theta}] - \theta = 0$
\subsection{Consistency}
An estimator is consistent iff
\[
\lim_{n \to \infty} P(\left| \hat{\theta} - \theta \right| < \epsilon) = 1 \text{ for } \epsilon > 0
\]
\section{Laws of Large Numbers}
\subsection{Weak}
For any $\epsilon > 0$
\[
P(\left| \overline{X} - \mu \right| \geq \epsilon) \to 0 \text{ as } n \to \infty
\]
\subsection{Strong}
\[
P\left( \lim_{n\to\infty} \left( X_1 + X_2 + ... + X_n \over n \right) = \mu \right) = 1
\]
\section{Inequalities}
\subsection{Markov's Inequality}
X is a non-negative RV
\[
P(X \geq a) \leq {E[X] \over a} \text{ for all } a > 0
\]
\subsection{Chebyshev's Inequality}
X is an RV. E[X] = $\mu$, Var(X) = $\sigma^2$
\[
P(\left|X - \mu\right| \geq k) \leq {\sigma^2 \over k^2} \text{ for all } k > 0
\]
\subsection{One-sided Chebyshev's Inequality}
X is an RV. E[X] = $\mu$, Var(X) = $\sigma^2$
\[
P(X \geq a) \leq {\sigma^2 \over \sigma^2 + a^2} \text{ for any } a > 0
\]
\subsection{Chernoff Bound}
M(t) is the MGF for RV X
\begin{align*}
&P(X \geq a) \leq e^{-ta} M(t) \text{ for all } t > 0 \\
&P(X \leq a) \leq e^{-ta} M(t) \text{ for all } t < 0
\end{align*}
\section{General Approach}
What's independent?\\
Where are there symmetries? (Can we break it up?)\\
Does it map to a known distribution?\\
Is the complement easier?\\
\end{document}
