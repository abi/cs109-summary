\documentclass[12pt]{amsart}
\usepackage[text={7in,10in}, centering]{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{CS 109 Course Summary}
\author{Daniel Jackoway}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

This document has formulas and other useful information about the first half of CS 109. The idea is that it should basically summarize everything we've learned so far and be super-valuable for the midterm.

\section{Counting}
\begin{align*}
{n \choose k} &= {n-1 \choose k-1} + {n-1 \choose k} \\
\end{align*}

\section{Probability Properties}
%
\subsection{Conditional Probability}
\[
P(E \mid F) = {P(EF) \over P(F)}
\]
\subsection{Chain Rule}
Implied by conditional probability
\begin{align*}
&P(EF) = P(E \mid F) P(F) \\
&\text{more generally:} \\
&P(E_1 E_2 E_3 ... E_n) = P(E_1) P(E_2 \mid E_1) P(E_3 \mid E_1 E_2) ... P(E_n \mid E_1 E_2 ... E_{n-1})
\end{align*}
%
\subsection{Bayes' Theorem} Lots of forms
\begin{align*}
P(F \mid E) &= {P(EF) \over P(E)} \\
&= {P(E \mid F) P(F) \over P(E)} \\
&= {P(E \mid F) P(F) \over P(E \mid F) P(F) + P(E \mid F^c) P(F^c)} \\
P(E) &= P(EF) + P(EF^c) \\
&= P(E \mid F) P(F) + P(E \mid F^c) P(F^c)
\end{align*}

\subsection{Odds}
\begin{align*}
&{P(A) \over P(A^c)} = {P(a) \over 1-P(A)} \\
&\text{After observing E, updated odds by multiplying by } {P(E \mid A) \over P(E \mid A^c)}
\end{align*}

\subsection{Conditional}
Can condition consistently on an event and commutativity, chain rule, intersection, and Bonferroni all still hold:
\begin{align*}
P(AB \mid E) &= P(BA \mid E) \\
P(AB \mid E) &= P(A \mid BE) P(B \mid E) \\
P(A \mid BE) &= {P(B \mid AE) P(A \mid E) \over P(B \mid E)}
\end{align*}

\subsection{Dependence}
E and F are independent iff $P(EF) = P(E) P(F)$. More generally:\\
$E_1, E_2, ..., E_n$ are independent iff for every subset $E_1 ... E_r$ (where $ r \leq n$), $P(E_1 E_2 ... E_r) = P(E_1) P(E_2) ... P(E_r)$

\section{Random Variables}
\subsection{Variance}
\begin{align*}
Var(X) &= E[(X - \mu)^2] = E[X^2] - (E[X])^2 \\
Var(aX + b) &= a^2 Var(X)
\end{align*}
%
\subsection{Binomial}
\subsubsection{What}
\[
X \sim \text{Bin}(n,p)
\]
Do n independent trials with probability of success p. X is the number of successes.
\subsubsection{Formulae}
\[
PMF: P(X = i) = p(i) = {n \choose i} p^i (1-p)^{n-i}
\]
\[
\text{CMF: sum the PMF from 0 to i}
\]
\[
E[X] = np
\]
\[
Var(X) = np(1-p)
\]
\subsubsection{Combining}
If they have the \textbf{same p}, you add their n's when you add them.
%
\subsection{Negative Binomial}
\subsubsection{What}
\[
X \sim NegBin(r, p)
\]
Do independent trials with probability p of success until r successes. X is the number of trials it takes.
\subsubsection{Formulae}
\[
P(X = n) = {n - 1 \choose r - 1} p^r (1-p)^{n-r}
\]
\[
E[X] = {r \over p}
\]
\[
Var(X) = {r(1-p) \over p^2}
\]
%
\subsection{Normal}
\subsubsection{What}
aka Gaussian. PDF is symmetric around $\mu$, peaks at $\mu$, ranges $-\infty, \infty$. Often comes from sum of multiple vars. Approximates binomial, best when $p = .5$
\subsubsection{Formulae}
\begin{align*}
X &\sim N(\mu, \sigma^2) \\
PDF: f(x) &= {1 \over \sigma \sqrt{2\pi}} e^{-(x-\mu)^2 / 2 \sigma^2} \\
E[X] &= \mu \\
Var(X) &= \sigma^2 \\
Bin(n, p) &\approx N(np, np(1-p)) & \text{if } np(1-p) \geq 10
\end{align*}
\subsubsection{Combining}
Sum the $\mu$'s and the $\sigma^2$'s when you add normal RV's to get the $\mu$ and the $\sigma^2$ of the variable equal to the sum.
%
\subsection{Exponential}
\subsubsection{What}
Models time until an event happens. "Memoryless"--distribution looking forward isn't affected by how long since the last event.
\subsubsection{Formulae}
\begin{align*}
X &\sim Exp(\lambda) \\
PDF: f(x) &= \lambda e^{-\lambda x} & \text{if x} \geq 0 \text{ (else 0)} \\
CDF: F(x) &= P(X \leq x) = 1 - e^{-\lambda x} & \text{where x} \geq 0 \\
E[X] &= {1 \over \lambda} \\
Var(X) &= {1 \over \lambda^2}
\end{align*}
%
\subsection{Geometric}
\subsubsection{What}
X is the number of independent trials (with probability p of success) until first success. $X \sim NegBin(1, p)$ See Negative Binomial.
\[
X \sim Geo(p)
\]
\subsubsection{Formulae}
\begin{align*}
P(X = n) &= (1-p)^{n-1} p \\
E[X] &= {1 \over p} \\
Var(X) &= {1 - p \over p^2}
\end{align*}
%
\subsection{Hypergeometric}
\subsubsection{What}
Urn with N balls, m white. Draw n balls without replacement. X is number of white balls drawn.
\[
X \sim HypG(n, N, m)
\]
\subsubsection{Formulae}
\begin{align*}
PMF: P(X = i) &= { {m \choose i} {N-m \choose n-i} \over {N \choose n}} &i=0,1,...,n \\
E[X] &= n \left( {m \over N} \right) \\
Var(X) &= {nm(N-n)(N-m) \over N^2 (N-1)} \\
\text{as } n &\to \infty, HypG(n, N, m) \to Bin(n, m/N)
\end{align*}
%
\subsection{Poisson}
\subsubsection{What}
$\lambda$ is the rate. eg. $\lambda = 5 \to 5$ occurrences happening per time period. Also approximates the binomial distribution when n is large, p is small, and $\lambda = np$ is moderate. In fact, the limit as n goes to infinity and p goes to 0 of the binomial is the poisson
\[
X \sim Poi(\lambda)
\]
\subsubsection{Formulae}
\begin{align*}
PMF: P(X=i) &= e^{-\lambda} {\lambda^i \over i!} \\
Bin(n, p) &\approx Poi(np) & \text{if } n > 100, p < .1 \text{ or } n > 20, p < .05
\end{align*}
\subsubsection{Combining}
Sum the $\lambda$'s when summing poisson RV's
%
\subsection{Uniform}
\subsubsection{What}
X is equally likely to be any number between $\alpha$ and $\beta$
\[
X \sim Uni(\alpha, \beta)
\]
\subsubsection{Formulae}
\begin{align*}
PDF: f(x) &= {1 \over \beta - \alpha} & \alpha < x < \beta \text{ (else 0)} \\
P(a \leq x \leq b) &= {b - a \over \beta - \alpha} \\
E[X] &= {\alpha + \beta \over 2} \\
Var(X) &= {(\beta - \alpha)^2 \over 12}
\end{align*}
%
\subsection{Beta}
\subsubsection{What}
Weird. Adapts as you change the params.\\
One use: flip a coin with unknown probability of coming up heads. Get n heads and m tails. The distribution suggested by that is $X \sim Beta(n + 1, m + 1)$. \\
symmetric when a = b.  High a shifts density right, high b shifts density left.
\[
X \sim Beta(a,b)
\]
\subsubsection{Formulae}
\begin{align*}
B(a,b) &= \int_0^1 x^{a-1} (1-x)^{b-1} dx \\
f(x) &= {1 \over B(a,b)} x^{a-1} (1-x)^{b-1} & 0 < x < 1 \text{ (else }0)\\
E[X] &= {a \over a + b} \\
Var(X) &= {ab \over (a + b)^2 (a + b + 1)}
\end{align*}
%
\subsection{Bernoulli}
\subsubsection{What}
1 with probability p and 0 with probability 1-p
\[
X \sim Ber(p)
\]
\subsubsection{Formulae}
\begin{align*}
E[X] &= p \\
Var(X) &= p(1-p)
\end{align*}

\end{document}
